{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce_memory\n",
    "def reduce_memory(df):\n",
    "    print(\"Reduce_memory...\");\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_day_feature(df, offset=0, tname='TransactionDT'):\n",
    "    \"\"\"\n",
    "    Creates a day of the week feature, encoded as 0-6. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        df to manipulate.\n",
    "    offset : float (default=0)\n",
    "        offset (in days) to shift the start/end of a day.\n",
    "    tname : str\n",
    "        Name of the time column in df.\n",
    "    \"\"\"\n",
    "    # found a good offset is 0.58\n",
    "    days = df[tname] / (3600*24)        \n",
    "    encoded_days = np.floor(days-1+offset) % 7\n",
    "    return encoded_days\n",
    "\n",
    "def make_hour_feature(df, tname='TransactionDT'):\n",
    "    \"\"\"\n",
    "    Creates an hour of the day feature, encoded as 0-23. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        df to manipulate.\n",
    "    tname : str\n",
    "        Name of the time column in df.\n",
    "    \"\"\"\n",
    "    hours = df[tname] / (3600)        \n",
    "    encoded_hours = np.floor(hours) % 24\n",
    "    return encoded_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_identity = pd.read_csv('train_identity.csv',index_col='TransactionID')\n",
    "train_transaction = pd.read_csv('train_transaction.csv',index_col='TransactionID')\n",
    "test_identity = pd.read_csv('test_identity.csv',index_col='TransactionID')\n",
    "test_transaction = pd.read_csv('test_transaction.csv',index_col='TransactionID')\n",
    "\n",
    "# Create train and test dataset by left outer join\n",
    "train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Delete variables to save memory\n",
    "del train_identity,train_transaction,test_identity,test_transaction\n",
    "\n",
    "y=train['isFraud'].astype('uint8')\n",
    "train.drop(['isFraud'], axis=1, inplace=True)\n",
    "\n",
    "# The column of \"TransactionDT\" is essentially a measure of time. It was found that the hours have some correlation with the fraud\n",
    "# 0.58 is recommended by a kaggle kernel to fit the meaning of transactional day.\n",
    "train['hours'] = make_hour_feature(train)\n",
    "test['hours'] = make_hour_feature(test)\n",
    "\n",
    "\n",
    "train.drop(['TransactionDT'], axis=1, inplace=True)\n",
    "test.drop(['TransactionDT'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of domains and countries from raw email data\n",
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n",
    "          'scranton.edu': 'other', 'optonline.net': 'other', \n",
    "          'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', \n",
    "          'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', \n",
    "          'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', \n",
    "          'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', \n",
    "          'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', \n",
    "          'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', \n",
    "          'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', \n",
    "          'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', \n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n",
    "          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', \n",
    "          'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', \n",
    "          'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n",
    "          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other',\n",
    "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other',\n",
    "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', \n",
    "          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    # Domain\n",
    "    train[c + '_bin'] = train[c].map(emails)\n",
    "    test[c + '_bin'] = test[c].map(emails)\n",
    "    \n",
    "    # Country\n",
    "    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {np.nan: 0, 'nan': 0}\n",
    "for c1, c2 in train.dtypes.reset_index().values:\n",
    "    if c2=='O':\n",
    "        for c in list(set(train[c1].unique())|set(test[c1].unique())):\n",
    "            if c not in labels:\n",
    "                labels[c] = len(labels) - 1\n",
    "\n",
    "for c1, c2 in train.dtypes.reset_index().values:\n",
    "    if c2=='O':\n",
    "        train[c1] = train[c1].map(lambda x: labels[str(x)])\n",
    "        test[c1] = test[c1].map(lambda x: labels[str(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to kaggel kernels, recommend dropping the following columns\n",
    "# Get duplicate columns\n",
    "duplicates = []\n",
    "cols = train.columns\n",
    "i = 0\n",
    "for c1 in cols:\n",
    "    i += 1\n",
    "    for c2 in cols[i:]:\n",
    "        if c1 != c2:\n",
    "            if (np.sum((train[c1].values == train[c2].values).astype(int)) / len(train))>0.95:\n",
    "                duplicates.append(c2)\n",
    "                print(c1, c2, np.sum((train[c1].values == train[c2].values).astype(int)) / len(train))\n",
    "\n",
    "duplicates = list(set(duplicates))\n",
    "print(duplicates)\n",
    "drop_col = duplicates\n",
    "\n",
    "\n",
    "# Explicitly list drop_col to save time\n",
    "# drop_col = ['V300', 'V309', 'V111', 'C3', 'V124', 'V106', \n",
    "#             'V125', 'V315', 'V134', 'V102', 'V123', 'V316', 'V113', 'V136', \n",
    "#             'V305', 'V110', 'V299', 'V289', 'V286', 'V318', 'V103', 'V304',\n",
    "#             'V116', 'V298', 'V284', 'V293', 'V137', 'V295', 'V301', 'V104', \n",
    "#             'V311', 'V115', 'V109', 'V119', 'V321', 'V114', 'V133', 'V122',\n",
    "#             'V319', 'V105', 'V112', 'V118', 'V117', 'V121', 'V108', 'V135',\n",
    "#             'V320', 'V303', 'V297', 'V120']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(drop_col , axis=1, inplace=True)\n",
    "test.drop(drop_col , axis=1, inplace=True)\n",
    "\n",
    "train_size = train.shape[0]\n",
    "test_size = test.shape[0]\n",
    "\n",
    "print('Max NA counts in train dataset is',train.isnull().sum().max())\n",
    "print('Max NA counts in test dataset is',test.isnull().sum().max())\n",
    "\n",
    "# Decision tree method dose not require feature scaling.\n",
    "# Label Encoding qualitative features (using labels shown above to encode for now)\n",
    "# for c in train.columns:\n",
    "#         if train[c].dtype=='object': \n",
    "#             lbl = preprocessing.LabelEncoder()\n",
    "#             lbl.fit(list(train[c].values)+list(test[c].values))\n",
    "#             train[c] = lbl.transform(list(train[c].values))\n",
    "#             test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "# Fill missing values after label encoding.\n",
    "# The values in the orginal datasets are all positive, so fill NA with a large negative number\n",
    "train = train.fillna(-999)\n",
    "test = test.fillna(-999)\n",
    "\n",
    "print('NA counts in train dataset now becomes',train.isnull().sum().max())\n",
    "print('NA counts in test dataset now becomes',test.isnull().sum().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing memory by change the dtypes of some columns\n",
    "train= reduce_memory(train)\n",
    "test= reduce_memory(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_path = './xgb_models_stack/'\n",
    "lgb_path = './lgb_models_stack/'\n",
    "\n",
    "# Create dir for models\n",
    "# os.mkdir(xgb_path)\n",
    "# os.mkdir(lgb_path)\n",
    "\n",
    "#XGBoost Model\n",
    "def fit_xgb(X_fit, y_fit, X_val, y_val, counter, xgb_path, name):\n",
    "    model = xgb.XGBClassifier(n_estimators=1000, max_depth=9, learning_rate=0.02, subsample=0.7, \n",
    "                              colsample_bytree=0.7,missing=-999,tree_method='hist')\n",
    "    model.fit(X_fit, y_fit,eval_set=[(X_val, y_val)],verbose=0,eval_metric=\"auc\",early_stopping_rounds=100)\n",
    "    cv_val = model.predict_proba(X_val)[:,1]\n",
    "    #Save XGBoost Model\n",
    "    save_to = '{}{}_fold{}.dat'.format(xgb_path, name, counter+1)\n",
    "    pickle.dump(model, open(save_to, \"wb\"))\n",
    "    del X_fit, y_fit, X_val, y_val\n",
    "    return cv_val\n",
    "\n",
    "#LightGBM Model\n",
    "def fit_lgb(X_fit, y_fit, X_val, y_val, counter, lgb_path, name):\n",
    "    model = lgb.LGBMClassifier(learning_rate=0.02,max_depth=9, boosting_type='gbdt',\n",
    "                               objective= 'binary', metric='auc', seed= 4, num_iterations= 2000,\n",
    "                               num_leaves= 64, feature_fraction= 0.4,\n",
    "                               bagging_fraction= 0.4, bagging_freq= 5)\n",
    "    model.fit(X_fit, y_fit,eval_set=[(X_val, y_val)],verbose=200,early_stopping_rounds=100)\n",
    "    cv_val = model.predict_proba(X_val)[:,1]\n",
    "    #Save LightGBM Model\n",
    "    save_to = '{}{}_fold{}.txt'.format(lgb_path, name, counter+1)\n",
    "    model.booster_.save_model(save_to)\n",
    "    del X_fit, y_fit, X_val, y_val\n",
    "    return cv_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation datasets from original train dataset\n",
    "X_train_, X_val_, y_train_, y_val_ = train_test_split(train, y, test_size=0.1, random_state=42)\n",
    "NumFold=5\n",
    "skf = StratifiedKFold(n_splits=NumFold, shuffle=True, random_state=42)\n",
    "# del train,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_cv_result = np.zeros(X_train_.shape[0])\n",
    "print('\\nModel Fitting...')\n",
    "for counter, (tr_idx, val_idx) in enumerate(skf.split(X_train_, y_train_)):\n",
    "    print('\\nFold {}'.format(counter+1))\n",
    "    X_fit, y_fit = X_train_.iloc[tr_idx,:], y_train_.iloc[tr_idx]\n",
    "    X_val, y_val = X_train_.iloc[val_idx,:], y_train_.iloc[val_idx]\n",
    "\n",
    "    print('XGBoost')\n",
    "    xgb_cv_result[val_idx] = fit_xgb(X_fit, y_fit, X_val, y_val, counter, lgb_path , name='xgb')\n",
    "\n",
    "    del X_fit, X_val, y_fit, y_val\n",
    "    # Free meomory by running garbarge collector\n",
    "    gc.collect()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_xgb  = round(roc_auc_score(y_train_, xgb_cv_result),4)\n",
    "print('\\nXGBoost  VAL AUC: {}'.format(auc_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_cv_result = np.zeros(X_train_.shape[0])\n",
    "for counter, (tr_idx, val_idx) in enumerate(skf.split(X_train_, y_train_)):\n",
    "    print('\\nFold {}'.format(counter+1))\n",
    "    X_fit, y_fit = X_train_.iloc[tr_idx,:], y_train_.iloc[tr_idx]\n",
    "    X_val, y_val = X_train_.iloc[val_idx,:], y_train_.iloc[val_idx]\n",
    "\n",
    "    print('LigthGBM')\n",
    "    lgb_cv_result[val_idx] = fit_lgb(X_fit, y_fit, X_val, y_val, counter, lgb_path, name='lgb')\n",
    "\n",
    "    del X_fit, X_val, y_fit, y_val\n",
    "    # Free meomory by running garbarge collector\n",
    "    gc.collect()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_lgb  = round(roc_auc_score(y_train_, lgb_cv_result),4)\n",
    "print('\\nLGBoost  TRAIN AUC: {}'.format(auc_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_models = sorted(os.listdir(xgb_path))\n",
    "xgb_result_val = np.zeros(X_val_.shape[0])\n",
    "xgb_result_test = np.zeros(test.shape[0])\n",
    "\n",
    "print('With XGBoost...')    \n",
    "for m_name in xgb_models:\n",
    "    #Load Xgboost Model\n",
    "    model = pickle.load(open('{}{}'.format(xgb_path, m_name), \"rb\"))\n",
    "    xgb_result_val += model.predict_proba(X_val_)[:,1]\n",
    "    xgb_result_test += model.predict_proba(test)[:,1]\n",
    "del model\n",
    "xgb_result_val /= len(xgb_models)\n",
    "xgb_result_test /= len(xgb_models)\n",
    "\n",
    "auc_xgb  = round(roc_auc_score(y_val_, xgb_result_val),4)\n",
    "print('\\nXGBoost  VAL AUC: {}'.format(auc_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "lgb_models = sorted(os.listdir(lgb_path))\n",
    "lgb_result_val = np.zeros(X_val_.shape[0])\n",
    "lgb_result_test = np.zeros(test.shape[0])\n",
    "\n",
    "print('With LightGBM...')   \n",
    "for m_name in lgb_models:\n",
    "    #Load LightGBM Model\n",
    "    model = lgb.Booster(model_file='{}{}'.format(lgb_path, m_name))\n",
    "    lgb_result_val += model.predict(X_val_)\n",
    "    lgb_result_test += model.predict(test)\n",
    "del model\n",
    "lgb_result_val /= len(lgb_models)\n",
    "lgb_result_test /= len(lgb_models)\n",
    "\n",
    "auc_lgb  = round(roc_auc_score(y_val_, lgb_result_val),4)\n",
    "print('\\nLGBoost  VAL AUC: {}'.format(auc_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submitting results\n",
    "submission = pd.read_csv('sample_submission.csv', index_col='TransactionID')\n",
    "submission['isFraud'] = lgb_result_test\n",
    "submission.to_csv('lgb_finer_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
